{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13296498,"sourceType":"datasetVersion","datasetId":8427398}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install sentence-transformers faiss-cpu pandas openpyxl PyPDF2 numpy","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-13T04:50:15.178748Z","iopub.execute_input":"2025-10-13T04:50:15.178988Z","iopub.status.idle":"2025-10-13T04:51:30.989645Z","shell.execute_reply.started":"2025-10-13T04:50:15.178960Z","shell.execute_reply":"2025-10-13T04:51:30.988945Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\nCollecting faiss-cpu\n  Downloading faiss_cpu-1.12.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\nCollecting PyPDF2\n  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.0.0rc2)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\nRequirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.15.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.19.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.9.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (0.28.1)\nRequirement already satisfied: typer-slim in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (0.19.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\nCollecting huggingface-hub>=0.20.0 (from sentence-transformers)\n  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.9.18)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.8.3)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (4.11.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (0.16.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (1.3.1)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (8.3.0)\nDownloading faiss_cpu-1.12.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m100.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: PyPDF2, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, nvidia-cusolver-cu12, faiss-cpu\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 1.0.0rc2\n    Uninstalling huggingface-hub-1.0.0rc2:\n      Successfully uninstalled huggingface-hub-1.0.0rc2\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed PyPDF2-3.0.1 faiss-cpu-1.12.0 huggingface-hub-0.35.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport pickle\nfrom pathlib import Path\nfrom typing import List, Dict, Any\nimport pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nimport faiss\nfrom PyPDF2 import PdfReader\nimport warnings\nimport logging","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T04:51:30.991366Z","iopub.execute_input":"2025-10-13T04:51:30.991571Z","iopub.status.idle":"2025-10-13T04:51:57.957318Z","shell.execute_reply.started":"2025-10-13T04:51:30.991553Z","shell.execute_reply":"2025-10-13T04:51:57.956719Z"}},"outputs":[{"name":"stderr","text":"2025-10-13 04:51:43.553257: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760331103.745937      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760331103.796911      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"warnings.filterwarnings('ignore')\nlogging.getLogger('sentence_transformers').setLevel(logging.ERROR)\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T04:51:57.958005Z","iopub.execute_input":"2025-10-13T04:51:57.958513Z","iopub.status.idle":"2025-10-13T04:51:57.962789Z","shell.execute_reply.started":"2025-10-13T04:51:57.958484Z","shell.execute_reply":"2025-10-13T04:51:57.962128Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class AgricultureVectorDB:\n    def __init__(self, base_folder: str, model_name: str = \"all-mpnet-base-v2\"):\n        \"\"\"\n        Initialize the vector database builder\n        \n        Args:\n            base_folder: Path to agriculture folder containing csvs/, pdfs/, excel/\n            model_name: Sentence transformer model (all-mpnet-base-v2 or all-MiniLM-L6-v2)\n        \"\"\"\n        self.base_folder = Path(base_folder)\n        \n        print(f\"Loading embedding model: {model_name}...\")\n        # Load model with trust_remote_code to avoid warnings\n        self.model = SentenceTransformer(model_name, trust_remote_code=True)\n        self.embedding_dim = self.model.get_sentence_embedding_dimension()\n        self.documents = []  # Store text chunks\n        self.metadata = []   # Store metadata for each chunk\n        self.index = None\n        \n        print(f\"✓ Loaded embedding model: {model_name}\")\n        print(f\"✓ Embedding dimension: {self.embedding_dim}\")\n    \n    def chunk_text(self, text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:\n        \"\"\"\n        Split text into overlapping chunks\n        \n        Args:\n            text: Input text\n            chunk_size: Target chunk size in characters\n            overlap: Overlap between chunks\n        \"\"\"\n        if len(text) <= chunk_size:\n            return [text]\n        \n        chunks = []\n        start = 0\n        while start < len(text):\n            end = start + chunk_size\n            chunk = text[start:end]\n            \n            # Try to break at sentence boundary\n            if end < len(text):\n                last_period = chunk.rfind('.')\n                last_newline = chunk.rfind('\\n')\n                break_point = max(last_period, last_newline)\n                if break_point > chunk_size * 0.5:  # At least 50% of chunk\n                    chunk = chunk[:break_point + 1]\n                    end = start + break_point + 1\n            \n            chunks.append(chunk.strip())\n            start = end - overlap\n        \n        return chunks\n    \n    def process_pdf(self, pdf_path: Path) -> None:\n        \"\"\"Extract and chunk text from PDF files (textbooks)\"\"\"\n        try:\n            reader = PdfReader(str(pdf_path))\n            print(f\"  Processing PDF: {pdf_path.name} ({len(reader.pages)} pages)\")\n            \n            for page_num, page in enumerate(reader.pages, 1):\n                text = page.extract_text()\n                if text.strip():\n                    chunks = self.chunk_text(text)\n                    for chunk_idx, chunk in enumerate(chunks):\n                        self.documents.append(chunk)\n                        self.metadata.append({\n                            'source': str(pdf_path.name),\n                            'type': 'pdf',\n                            'page': page_num,\n                            'chunk': chunk_idx,\n                            'path': str(pdf_path)\n                        })\n        except Exception as e:\n            print(f\"  ✗ Error processing {pdf_path.name}: {str(e)}\")\n    \n    def process_csv(self, csv_path: Path) -> None:\n        \"\"\"Process CSV with numerical data and headers\"\"\"\n        try:\n            df = pd.read_csv(csv_path)\n            print(f\"  Processing CSV: {csv_path.name} ({len(df)} rows, {len(df.columns)} columns)\")\n            \n            # Create text representation of each row\n            for idx, row in df.iterrows():\n                # Combine column names with values for context\n                row_text = f\"Data from {csv_path.stem}:\\n\"\n                for col in df.columns:\n                    value = row[col]\n                    if pd.notna(value):\n                        row_text += f\"{col}: {value}\\n\"\n                \n                # Chunk if row text is too long\n                chunks = self.chunk_text(row_text, chunk_size=800)\n                for chunk_idx, chunk in enumerate(chunks):\n                    self.documents.append(chunk)\n                    self.metadata.append({\n                        'source': str(csv_path.name),\n                        'type': 'csv',\n                        'row': idx,\n                        'chunk': chunk_idx,\n                        'path': str(csv_path)\n                    })\n        except Exception as e:\n            print(f\"  ✗ Error processing {csv_path.name}: {str(e)}\")\n    \n    def process_excel(self, excel_path: Path) -> None:\n        \"\"\"Process Excel files with numerical data\"\"\"\n        try:\n            # Read all sheets\n            excel_file = pd.ExcelFile(excel_path)\n            print(f\"  Processing Excel: {excel_path.name} ({len(excel_file.sheet_names)} sheets)\")\n            \n            for sheet_name in excel_file.sheet_names:\n                df = pd.read_excel(excel_path, sheet_name=sheet_name)\n                \n                for idx, row in df.iterrows():\n                    row_text = f\"Data from {excel_path.stem} - Sheet: {sheet_name}:\\n\"\n                    for col in df.columns:\n                        value = row[col]\n                        if pd.notna(value):\n                            row_text += f\"{col}: {value}\\n\"\n                    \n                    chunks = self.chunk_text(row_text, chunk_size=800)\n                    for chunk_idx, chunk in enumerate(chunks):\n                        self.documents.append(chunk)\n                        self.metadata.append({\n                            'source': str(excel_path.name),\n                            'type': 'excel',\n                            'sheet': sheet_name,\n                            'row': idx,\n                            'chunk': chunk_idx,\n                            'path': str(excel_path)\n                        })\n        except Exception as e:\n            print(f\"  ✗ Error processing {excel_path.name}: {str(e)}\")\n    \n    def load_all_documents(self) -> None:\n        \"\"\"Load and process all documents from the folder structure\"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"LOADING DOCUMENTS\")\n        print(\"=\"*60)\n        \n        # Process PDFs\n        pdf_folder = self.base_folder / \"pdfs\"\n        if pdf_folder.exists():\n            print(\"\\n📄 Processing PDFs...\")\n            for pdf_file in pdf_folder.glob(\"*.pdf\"):\n                self.process_pdf(pdf_file)\n        \n        # Process CSVs\n        csv_folder = self.base_folder / \"csvs\"\n        if csv_folder.exists():\n            print(\"\\n📊 Processing CSVs...\")\n            for csv_file in csv_folder.glob(\"*.csv\"):\n                self.process_csv(csv_file)\n        \n        # Process Excel files\n        excel_folder = self.base_folder / \"excel\"\n        if excel_folder.exists():\n            print(\"\\n📈 Processing Excel files...\")\n            for excel_file in excel_folder.glob(\"*.xlsx\"):\n                self.process_excel(excel_file)\n            for excel_file in excel_folder.glob(\"*.xls\"):\n                self.process_excel(excel_file)\n        \n        print(f\"\\n✓ Total documents loaded: {len(self.documents)}\")\n        print(f\"✓ Total chunks created: {len(self.documents)}\")\n    \n    def build_index(self, batch_size: int = 32) -> None:\n        \"\"\"Build FAISS index from documents\"\"\"\n        if not self.documents:\n            raise ValueError(\"No documents loaded. Call load_all_documents() first.\")\n        \n        print(\"\\n\" + \"=\"*60)\n        print(\"BUILDING FAISS INDEX\")\n        print(\"=\"*60)\n        \n        # Generate embeddings in batches\n        print(f\"\\nGenerating embeddings for {len(self.documents)} chunks...\")\n        embeddings = []\n        \n        for i in range(0, len(self.documents), batch_size):\n            batch = self.documents[i:i + batch_size]\n            batch_embeddings = self.model.encode(batch, show_progress_bar=False)\n            embeddings.append(batch_embeddings)\n            if (i // batch_size + 1) % 10 == 0:\n                print(f\"  Processed {i + len(batch)}/{len(self.documents)} chunks...\")\n        \n        embeddings = np.vstack(embeddings).astype('float32')\n        print(f\"✓ Embeddings shape: {embeddings.shape}\")\n        \n        # Create FAISS index\n        print(\"\\nCreating FAISS index...\")\n        self.index = faiss.IndexFlatL2(self.embedding_dim)\n        self.index.add(embeddings)\n        print(f\"✓ FAISS index created with {self.index.ntotal} vectors\")\n    \n    def save(self, output_dir: str = \"faiss_index\") -> None:\n        \"\"\"Save FAISS index and metadata\"\"\"\n        output_path = Path(output_dir)\n        output_path.mkdir(exist_ok=True)\n        \n        print(\"\\n\" + \"=\"*60)\n        print(\"SAVING INDEX\")\n        print(\"=\"*60)\n        \n        # Save FAISS index\n        index_path = output_path / \"faiss_index.bin\"\n        faiss.write_index(self.index, str(index_path))\n        print(f\"✓ FAISS index saved to: {index_path}\")\n        \n        # Save documents and metadata\n        data_path = output_path / \"documents_metadata.pkl\"\n        with open(data_path, 'wb') as f:\n            pickle.dump({\n                'documents': self.documents,\n                'metadata': self.metadata,\n                'model_name': self.model.get_sentence_embedding_dimension()\n            }, f)\n        print(f\"✓ Documents and metadata saved to: {data_path}\")\n        \n        # Save summary\n        summary_path = output_path / \"index_summary.txt\"\n        with open(summary_path, 'w') as f:\n            f.write(\"FAISS Vector Database Summary\\n\")\n            f.write(\"=\"*50 + \"\\n\\n\")\n            f.write(f\"Total chunks: {len(self.documents)}\\n\")\n            f.write(f\"Embedding dimension: {self.embedding_dim}\\n\")\n            f.write(f\"Index size: {self.index.ntotal}\\n\\n\")\n            \n            # Count by file type\n            pdf_count = sum(1 for m in self.metadata if m['type'] == 'pdf')\n            csv_count = sum(1 for m in self.metadata if m['type'] == 'csv')\n            excel_count = sum(1 for m in self.metadata if m['type'] == 'excel')\n            \n            f.write(f\"PDF chunks: {pdf_count}\\n\")\n            f.write(f\"CSV chunks: {csv_count}\\n\")\n            f.write(f\"Excel chunks: {excel_count}\\n\")\n        \n        print(f\"✓ Summary saved to: {summary_path}\")\n    \n    def load(self, index_dir: str = \"faiss_index\") -> None:\n        \"\"\"Load existing FAISS index\"\"\"\n        index_path = Path(index_dir)\n        \n        # Load FAISS index\n        self.index = faiss.read_index(str(index_path / \"faiss_index.bin\"))\n        \n        # Load documents and metadata\n        with open(index_path / \"documents_metadata.pkl\", 'rb') as f:\n            data = pickle.load(f)\n            self.documents = data['documents']\n            self.metadata = data['metadata']\n        \n        print(f\"✓ Loaded index with {self.index.ntotal} vectors\")\n    \n    def search(self, query: str, k: int = 5) -> List[Dict[str, Any]]:\n        \"\"\"\n        Search the vector database\n        \n        Args:\n            query: Search query\n            k: Number of results to return\n        \n        Returns:\n            List of dictionaries containing document text, metadata, and similarity score\n        \"\"\"\n        if self.index is None:\n            raise ValueError(\"Index not built. Call build_index() or load() first.\")\n        \n        # Generate query embedding\n        query_embedding = self.model.encode([query]).astype('float32')\n        \n        # Search\n        distances, indices = self.index.search(query_embedding, k)\n        \n        results = []\n        for dist, idx in zip(distances[0], indices[0]):\n            results.append({\n                'text': self.documents[idx],\n                'metadata': self.metadata[idx],\n                'similarity_score': float(1 / (1 + dist))  # Convert distance to similarity\n            })\n        \n        return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T04:51:57.963457Z","iopub.execute_input":"2025-10-13T04:51:57.963761Z","iopub.status.idle":"2025-10-13T04:51:57.992852Z","shell.execute_reply.started":"2025-10-13T04:51:57.963727Z","shell.execute_reply":"2025-10-13T04:51:57.992193Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Initialize\n    db = AgricultureVectorDB(\n        base_folder=\"/kaggle/input/agriculture/New folder\",\n        model_name=\"all-mpnet-base-v2\"  # or \"all-MiniLM-L6-v2\" for faster processing\n    )\n    \n    # Load all documents\n    db.load_all_documents()\n    \n    # Build FAISS index\n    db.build_index()\n    \n    # Save index\n    db.save(output_dir=\"faiss_index\")\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"✓ VECTOR DATABASE BUILD COMPLETE!\")\n    print(\"=\"*60)\n    \n    # Test search\n    print(\"\\n📝 Testing search functionality...\")\n    results = db.search(\"crop cultivation techniques\", k=3)\n    \n    print(\"\\nTop 3 results:\")\n    for i, result in enumerate(results, 1):\n        print(f\"\\n{i}. Source: {result['metadata']['source']}\")\n        print(f\"   Type: {result['metadata']['type']}\")\n        print(f\"   Score: {result['similarity_score']:.4f}\")\n        print(f\"   Text preview: {result['text'][:200]}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T04:51:57.993672Z","iopub.execute_input":"2025-10-13T04:51:57.993963Z","iopub.status.idle":"2025-10-13T05:07:48.342347Z","shell.execute_reply.started":"2025-10-13T04:51:57.993938Z","shell.execute_reply":"2025-10-13T05:07:48.341621Z"}},"outputs":[{"name":"stdout","text":"Loading embedding model: all-mpnet-base-v2...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7547c780e6ae4cfca41df36558c7f0e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d4d7514de1a48bc86356886354e8551"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b11a10536fe42628ddf49d25ba70644"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63fed3fd923848adae716e86581dd99a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01b2bac7457d4311a911764f5654eaab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"425adc711d7d4a1695235d9ca316f42f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37d011f896644ceb8d4bd55cbd86c7c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5ab06bc57564fe2b515de4e791acc2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a71ecde4775414293d68dcdc2e5b120"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b025c5584922488daea1d56af00da7d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81bf2f05dcc24a4691e0a0c3cac96af0"}},"metadata":{}},{"name":"stdout","text":"✓ Loaded embedding model: all-mpnet-base-v2\n✓ Embedding dimension: 768\n\n============================================================\nLOADING DOCUMENTS\n============================================================\n\n📄 Processing PDFs...\n  Processing PDF: agronomy_textbook.pdf (856 pages)\n  Processing PDF: indian_agriculture_after_independence.pdf (447 pages)\n  Processing PDF: basic_agriculture_cbse.pdf (208 pages)\n  Processing PDF: ncert_agriculture_textbook.pdf (10 pages)\n\n📊 Processing CSVs...\n  Processing CSV: upag_crop_data.csv (104 rows, 12 columns)\n  Processing CSV: Percentage_Participation_Report_2022_23_0.csv (70 rows, 7 columns)\n  Processing CSV: des_crop_data.csv (1459 rows, 105 columns)\n  Processing CSV: Animal Dataset.csv (205 rows, 16 columns)\n  Processing CSV: data_core.csv (8000 rows, 9 columns)\n  Processing CSV: all_agriculture_india.csv (2238 rows, 9 columns)\n  Processing CSV: RS_Session_267_AU_2998_3.csv (33 rows, 8 columns)\n  Processing CSV: indian_agriculture_dataset.csv (16146 rows, 80 columns)\n\n📈 Processing Excel files...\n  Processing Excel: EXPORT_OF_HORTICULTURE_PRODUCE_IN_INDIA.xls (1 sheets)\n  Processing Excel: Horticultural_Crops_Varieties_Certified_List_2002-09.xls (1 sheets)\n  Processing Excel: Horticultural_Crops_Varieties_Central_Released_2005-09.xls (1 sheets)\n  Processing Excel: AREA_AND_PRODUCTION_OF_VARIOUS_HORTICULTURE_CROPS.xls (2 sheets)\n\n✓ Total documents loaded: 99271\n✓ Total chunks created: 99271\n\n============================================================\nBUILDING FAISS INDEX\n============================================================\n\nGenerating embeddings for 99271 chunks...\n  Processed 320/99271 chunks...\n  Processed 640/99271 chunks...\n  Processed 960/99271 chunks...\n  Processed 1280/99271 chunks...\n  Processed 1600/99271 chunks...\n  Processed 1920/99271 chunks...\n  Processed 2240/99271 chunks...\n  Processed 2560/99271 chunks...\n  Processed 2880/99271 chunks...\n  Processed 3200/99271 chunks...\n  Processed 3520/99271 chunks...\n  Processed 3840/99271 chunks...\n  Processed 4160/99271 chunks...\n  Processed 4480/99271 chunks...\n  Processed 4800/99271 chunks...\n  Processed 5120/99271 chunks...\n  Processed 5440/99271 chunks...\n  Processed 5760/99271 chunks...\n  Processed 6080/99271 chunks...\n  Processed 6400/99271 chunks...\n  Processed 6720/99271 chunks...\n  Processed 7040/99271 chunks...\n  Processed 7360/99271 chunks...\n  Processed 7680/99271 chunks...\n  Processed 8000/99271 chunks...\n  Processed 8320/99271 chunks...\n  Processed 8640/99271 chunks...\n  Processed 8960/99271 chunks...\n  Processed 9280/99271 chunks...\n  Processed 9600/99271 chunks...\n  Processed 9920/99271 chunks...\n  Processed 10240/99271 chunks...\n  Processed 10560/99271 chunks...\n  Processed 10880/99271 chunks...\n  Processed 11200/99271 chunks...\n  Processed 11520/99271 chunks...\n  Processed 11840/99271 chunks...\n  Processed 12160/99271 chunks...\n  Processed 12480/99271 chunks...\n  Processed 12800/99271 chunks...\n  Processed 13120/99271 chunks...\n  Processed 13440/99271 chunks...\n  Processed 13760/99271 chunks...\n  Processed 14080/99271 chunks...\n  Processed 14400/99271 chunks...\n  Processed 14720/99271 chunks...\n  Processed 15040/99271 chunks...\n  Processed 15360/99271 chunks...\n  Processed 15680/99271 chunks...\n  Processed 16000/99271 chunks...\n  Processed 16320/99271 chunks...\n  Processed 16640/99271 chunks...\n  Processed 16960/99271 chunks...\n  Processed 17280/99271 chunks...\n  Processed 17600/99271 chunks...\n  Processed 17920/99271 chunks...\n  Processed 18240/99271 chunks...\n  Processed 18560/99271 chunks...\n  Processed 18880/99271 chunks...\n  Processed 19200/99271 chunks...\n  Processed 19520/99271 chunks...\n  Processed 19840/99271 chunks...\n  Processed 20160/99271 chunks...\n  Processed 20480/99271 chunks...\n  Processed 20800/99271 chunks...\n  Processed 21120/99271 chunks...\n  Processed 21440/99271 chunks...\n  Processed 21760/99271 chunks...\n  Processed 22080/99271 chunks...\n  Processed 22400/99271 chunks...\n  Processed 22720/99271 chunks...\n  Processed 23040/99271 chunks...\n  Processed 23360/99271 chunks...\n  Processed 23680/99271 chunks...\n  Processed 24000/99271 chunks...\n  Processed 24320/99271 chunks...\n  Processed 24640/99271 chunks...\n  Processed 24960/99271 chunks...\n  Processed 25280/99271 chunks...\n  Processed 25600/99271 chunks...\n  Processed 25920/99271 chunks...\n  Processed 26240/99271 chunks...\n  Processed 26560/99271 chunks...\n  Processed 26880/99271 chunks...\n  Processed 27200/99271 chunks...\n  Processed 27520/99271 chunks...\n  Processed 27840/99271 chunks...\n  Processed 28160/99271 chunks...\n  Processed 28480/99271 chunks...\n  Processed 28800/99271 chunks...\n  Processed 29120/99271 chunks...\n  Processed 29440/99271 chunks...\n  Processed 29760/99271 chunks...\n  Processed 30080/99271 chunks...\n  Processed 30400/99271 chunks...\n  Processed 30720/99271 chunks...\n  Processed 31040/99271 chunks...\n  Processed 31360/99271 chunks...\n  Processed 31680/99271 chunks...\n  Processed 32000/99271 chunks...\n  Processed 32320/99271 chunks...\n  Processed 32640/99271 chunks...\n  Processed 32960/99271 chunks...\n  Processed 33280/99271 chunks...\n  Processed 33600/99271 chunks...\n  Processed 33920/99271 chunks...\n  Processed 34240/99271 chunks...\n  Processed 34560/99271 chunks...\n  Processed 34880/99271 chunks...\n  Processed 35200/99271 chunks...\n  Processed 35520/99271 chunks...\n  Processed 35840/99271 chunks...\n  Processed 36160/99271 chunks...\n  Processed 36480/99271 chunks...\n  Processed 36800/99271 chunks...\n  Processed 37120/99271 chunks...\n  Processed 37440/99271 chunks...\n  Processed 37760/99271 chunks...\n  Processed 38080/99271 chunks...\n  Processed 38400/99271 chunks...\n  Processed 38720/99271 chunks...\n  Processed 39040/99271 chunks...\n  Processed 39360/99271 chunks...\n  Processed 39680/99271 chunks...\n  Processed 40000/99271 chunks...\n  Processed 40320/99271 chunks...\n  Processed 40640/99271 chunks...\n  Processed 40960/99271 chunks...\n  Processed 41280/99271 chunks...\n  Processed 41600/99271 chunks...\n  Processed 41920/99271 chunks...\n  Processed 42240/99271 chunks...\n  Processed 42560/99271 chunks...\n  Processed 42880/99271 chunks...\n  Processed 43200/99271 chunks...\n  Processed 43520/99271 chunks...\n  Processed 43840/99271 chunks...\n  Processed 44160/99271 chunks...\n  Processed 44480/99271 chunks...\n  Processed 44800/99271 chunks...\n  Processed 45120/99271 chunks...\n  Processed 45440/99271 chunks...\n  Processed 45760/99271 chunks...\n  Processed 46080/99271 chunks...\n  Processed 46400/99271 chunks...\n  Processed 46720/99271 chunks...\n  Processed 47040/99271 chunks...\n  Processed 47360/99271 chunks...\n  Processed 47680/99271 chunks...\n  Processed 48000/99271 chunks...\n  Processed 48320/99271 chunks...\n  Processed 48640/99271 chunks...\n  Processed 48960/99271 chunks...\n  Processed 49280/99271 chunks...\n  Processed 49600/99271 chunks...\n  Processed 49920/99271 chunks...\n  Processed 50240/99271 chunks...\n  Processed 50560/99271 chunks...\n  Processed 50880/99271 chunks...\n  Processed 51200/99271 chunks...\n  Processed 51520/99271 chunks...\n  Processed 51840/99271 chunks...\n  Processed 52160/99271 chunks...\n  Processed 52480/99271 chunks...\n  Processed 52800/99271 chunks...\n  Processed 53120/99271 chunks...\n  Processed 53440/99271 chunks...\n  Processed 53760/99271 chunks...\n  Processed 54080/99271 chunks...\n  Processed 54400/99271 chunks...\n  Processed 54720/99271 chunks...\n  Processed 55040/99271 chunks...\n  Processed 55360/99271 chunks...\n  Processed 55680/99271 chunks...\n  Processed 56000/99271 chunks...\n  Processed 56320/99271 chunks...\n  Processed 56640/99271 chunks...\n  Processed 56960/99271 chunks...\n  Processed 57280/99271 chunks...\n  Processed 57600/99271 chunks...\n  Processed 57920/99271 chunks...\n  Processed 58240/99271 chunks...\n  Processed 58560/99271 chunks...\n  Processed 58880/99271 chunks...\n  Processed 59200/99271 chunks...\n  Processed 59520/99271 chunks...\n  Processed 59840/99271 chunks...\n  Processed 60160/99271 chunks...\n  Processed 60480/99271 chunks...\n  Processed 60800/99271 chunks...\n  Processed 61120/99271 chunks...\n  Processed 61440/99271 chunks...\n  Processed 61760/99271 chunks...\n  Processed 62080/99271 chunks...\n  Processed 62400/99271 chunks...\n  Processed 62720/99271 chunks...\n  Processed 63040/99271 chunks...\n  Processed 63360/99271 chunks...\n  Processed 63680/99271 chunks...\n  Processed 64000/99271 chunks...\n  Processed 64320/99271 chunks...\n  Processed 64640/99271 chunks...\n  Processed 64960/99271 chunks...\n  Processed 65280/99271 chunks...\n  Processed 65600/99271 chunks...\n  Processed 65920/99271 chunks...\n  Processed 66240/99271 chunks...\n  Processed 66560/99271 chunks...\n  Processed 66880/99271 chunks...\n  Processed 67200/99271 chunks...\n  Processed 67520/99271 chunks...\n  Processed 67840/99271 chunks...\n  Processed 68160/99271 chunks...\n  Processed 68480/99271 chunks...\n  Processed 68800/99271 chunks...\n  Processed 69120/99271 chunks...\n  Processed 69440/99271 chunks...\n  Processed 69760/99271 chunks...\n  Processed 70080/99271 chunks...\n  Processed 70400/99271 chunks...\n  Processed 70720/99271 chunks...\n  Processed 71040/99271 chunks...\n  Processed 71360/99271 chunks...\n  Processed 71680/99271 chunks...\n  Processed 72000/99271 chunks...\n  Processed 72320/99271 chunks...\n  Processed 72640/99271 chunks...\n  Processed 72960/99271 chunks...\n  Processed 73280/99271 chunks...\n  Processed 73600/99271 chunks...\n  Processed 73920/99271 chunks...\n  Processed 74240/99271 chunks...\n  Processed 74560/99271 chunks...\n  Processed 74880/99271 chunks...\n  Processed 75200/99271 chunks...\n  Processed 75520/99271 chunks...\n  Processed 75840/99271 chunks...\n  Processed 76160/99271 chunks...\n  Processed 76480/99271 chunks...\n  Processed 76800/99271 chunks...\n  Processed 77120/99271 chunks...\n  Processed 77440/99271 chunks...\n  Processed 77760/99271 chunks...\n  Processed 78080/99271 chunks...\n  Processed 78400/99271 chunks...\n  Processed 78720/99271 chunks...\n  Processed 79040/99271 chunks...\n  Processed 79360/99271 chunks...\n  Processed 79680/99271 chunks...\n  Processed 80000/99271 chunks...\n  Processed 80320/99271 chunks...\n  Processed 80640/99271 chunks...\n  Processed 80960/99271 chunks...\n  Processed 81280/99271 chunks...\n  Processed 81600/99271 chunks...\n  Processed 81920/99271 chunks...\n  Processed 82240/99271 chunks...\n  Processed 82560/99271 chunks...\n  Processed 82880/99271 chunks...\n  Processed 83200/99271 chunks...\n  Processed 83520/99271 chunks...\n  Processed 83840/99271 chunks...\n  Processed 84160/99271 chunks...\n  Processed 84480/99271 chunks...\n  Processed 84800/99271 chunks...\n  Processed 85120/99271 chunks...\n  Processed 85440/99271 chunks...\n  Processed 85760/99271 chunks...\n  Processed 86080/99271 chunks...\n  Processed 86400/99271 chunks...\n  Processed 86720/99271 chunks...\n  Processed 87040/99271 chunks...\n  Processed 87360/99271 chunks...\n  Processed 87680/99271 chunks...\n  Processed 88000/99271 chunks...\n  Processed 88320/99271 chunks...\n  Processed 88640/99271 chunks...\n  Processed 88960/99271 chunks...\n  Processed 89280/99271 chunks...\n  Processed 89600/99271 chunks...\n  Processed 89920/99271 chunks...\n  Processed 90240/99271 chunks...\n  Processed 90560/99271 chunks...\n  Processed 90880/99271 chunks...\n  Processed 91200/99271 chunks...\n  Processed 91520/99271 chunks...\n  Processed 91840/99271 chunks...\n  Processed 92160/99271 chunks...\n  Processed 92480/99271 chunks...\n  Processed 92800/99271 chunks...\n  Processed 93120/99271 chunks...\n  Processed 93440/99271 chunks...\n  Processed 93760/99271 chunks...\n  Processed 94080/99271 chunks...\n  Processed 94400/99271 chunks...\n  Processed 94720/99271 chunks...\n  Processed 95040/99271 chunks...\n  Processed 95360/99271 chunks...\n  Processed 95680/99271 chunks...\n  Processed 96000/99271 chunks...\n  Processed 96320/99271 chunks...\n  Processed 96640/99271 chunks...\n  Processed 96960/99271 chunks...\n  Processed 97280/99271 chunks...\n  Processed 97600/99271 chunks...\n  Processed 97920/99271 chunks...\n  Processed 98240/99271 chunks...\n  Processed 98560/99271 chunks...\n  Processed 98880/99271 chunks...\n  Processed 99200/99271 chunks...\n✓ Embeddings shape: (99271, 768)\n\nCreating FAISS index...\n✓ FAISS index created with 99271 vectors\n\n============================================================\nSAVING INDEX\n============================================================\n✓ FAISS index saved to: faiss_index/faiss_index.bin\n✓ Documents and metadata saved to: faiss_index/documents_metadata.pkl\n✓ Summary saved to: faiss_index/index_summary.txt\n\n============================================================\n✓ VECTOR DATABASE BUILD COMPLETE!\n============================================================\n\n📝 Testing search functionality...\n\nTop 3 results:\n\n1. Source: basic_agriculture_cbse.pdf\n   Type: pdf\n   Score: 0.6840\n   Text preview: ulture in the developing world.\nCrop Production Practices\nCrop production practices are of utmost importance for successful and economic cultivation of field crops and national food security at large....\n\n2. Source: agronomy_textbook.pdf\n   Type: pdf\n   Score: 0.6814\n   Text preview: CROPPING  SYSTEM  AND FARMING  SYSTEM 679\nthermo-insensitivity high yielding varieties while cropping systems under rainfed conditions have been\ndeveloped depending on availability of cultivars w ith ...\n\n3. Source: agronomy_textbook.pdf\n   Type: pdf\n   Score: 0.6730\n   Text preview: Chapter 16\nCropping System and\nFarming System\nIn the recent past, cropping systems approach has gained importance in agr iculture and related enter-\nprises. A system consists of several components, wh...\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}